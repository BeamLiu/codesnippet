# StyleMIDI ğŸµ

*English | [ä¸­æ–‡(Chinese)](#stylemidi-ä¸­æ–‡ç‰ˆ)*

**Stylized AI Music Composition Engine**
*Building a Transformer from Scratch Â· Training Â· Inference Â· Web Demo*

StyleMIDI is a stylized music generation system built entirely from scratch based on the Transformer architecture. It learns the creative styles of specific composers (e.g., Beethoven, Chopin) and automatically generates MIDI music corresponding to given structured text conditions (Composer, Mood, Tempo, Key).

---

## ğŸŒŸ Core Features

- **Built from Scratch**: Fully implements a Transformer Decoder including Multi-Head Attention, RoPE (Rotary Position Embedding), and KV Cache.
- **Innovative Conditioning**: Utilizes REMI encoding combined with multi-conditional Tokens to control musical style, eliminating the need for a pre-trained text encoder.
- **Automated Data Augmentation/Mining**: Extracts Key, Mood, and Tempo labels automatically from MAESTRO MIDI data using music theory algorithms to balance data distribution.
- **Interactive Web UI**: A React-based interface with a FastAPI backend, allowing one-click generation, audio playback, and piano roll visualization.
- **Hardware Friendly**: Lightweight model (~25M parameters) that can be fully trained on a consumer GPU (e.g., RTX 3060 8GB) and inferred on CPU.

## ğŸ—ï¸ Architecture

The system flows through four main layers:
1. **Data Layer**: MIDI Parsing â†’ REMI Encoding â†’ Dataset (`pretty_midi`, `miditok`)
2. **Model Layer**: Custom Transformer Decoder (PyTorch, without using `nn.Transformer`)
3. **Inference Layer**: Conditional Sampling â†’ MIDI Generation (CPU Inference, KV Cache acceleration)
4. **Presentation Layer**: Web UI + Visualization (React, FastAPI, HTML Canvas / `html-midi-player`)

### Model Specifications
- **Layers**: 6
- **Attention Heads**: 8
- **Hidden Dimension**: 512
- **Max Sequence Length**: 1024
- **Vocabulary Size**: ~400 (REMI Tokens + Condition Tokens)

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/StyleMIDI.git
cd StyleMIDI

# Install dependencies
pip install -r requirements.txt
```

### Web Interface & Inference

The project relies on a React frontend and a FastAPI backend for inference:

**1. Start the Backend API**
```bash
# From the project root
python api/server.py
```

**2. Start the Frontend UI**
```bash
cd style_midi_ui
npm install
npm run dev
```
*Open the provided local Vite URL (e.g., http://localhost:5173) in your browser, select a composer, adjust sliders, and generate!*

### Training

To train the model from scratch using the MAESTRO dataset:

```bash
# Download dataset, extract features & prepare tokens
python scripts/prepare_data.py

# Start training (reference: result/style_midi_train.ipynb)
python src/train.py
```

*Note: Training results and reference outputs (like notebooks, charts, and weights) are saved in the `result/` directory.*

---

<br>
<br>

# StyleMIDI (ä¸­æ–‡ç‰ˆ) ğŸµ

**é£æ ¼åŒ– AI ä½œæ›²å¼•æ“**
*ä»é›¶å®ç° Transformer Â· è®­ç»ƒ Â· æ¨ç† Â· Web Demo*

StyleMIDI æ˜¯ä¸€ä¸ªä»é›¶å®ç°çš„é£æ ¼åŒ–éŸ³ä¹ç”Ÿæˆç³»ç»Ÿï¼ŒåŸºäº Transformer æ¶æ„ï¼Œèƒ½å¤Ÿå­¦ä¹ æŒ‡å®šä½œæ›²å®¶ï¼ˆå¦‚è´å¤šèŠ¬ã€è‚–é‚¦ï¼‰çš„åˆ›ä½œé£æ ¼ï¼Œå¹¶æ ¹æ®ç»“æ„åŒ–æ–‡æœ¬æ¡ä»¶ï¼ˆä½œæ›²å®¶ã€æƒ…ç»ªã€é€Ÿåº¦ã€è°ƒæ€§ï¼‰è‡ªåŠ¨ç”Ÿæˆå¯¹åº”é£æ ¼çš„ MIDI éŸ³ä¹ã€‚

---

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

- **ä»é›¶æ„å»º**: çº¯æ‰‹å·¥æ‰“é€  Transformer Decoderï¼ŒåŒ…å« Multi-Head Attentionã€RoPE (æ—‹è½¬ä½ç½®ç¼–ç ) å’Œ KV Cacheã€‚
- **åˆ›æ–°æ¡ä»¶æ§åˆ¶**: é‡‡ç”¨ REMI ç¼–ç  + å¤šæ¡ä»¶ Token æ§åˆ¶é£æ ¼ï¼Œæ— éœ€ä¾èµ–é¢„è®­ç»ƒçš„ NLP æ–‡æœ¬ç¼–ç å™¨ã€‚
- **è‡ªåŠ¨åŒ–æ•°æ®æŒ–æ˜ä¸å¢å¼º**: é€šè¿‡ä¹ç†ç®—æ³•è‡ªåŠ¨ä» MAESTRO æ•°æ®é›†ä¸­æŒ–æ˜è°ƒæ€§(Key)ã€æƒ…ç»ª(Mood)ä¸é€Ÿåº¦(Tempo)æ ‡ç­¾ï¼Œè§£å†³æ•°æ®åˆ†å¸ƒä¸å‡è¡¡é—®é¢˜ã€‚
- **å…æ˜¾å¡æ¨ç†ä¸äº¤äº’ UI**: æ¨¡å‹ä»… ~25M å‚æ•°ï¼Œæ”¯æŒå•å¡ RTX 3060 å®Œæ•´è®­ç»ƒï¼Œå¹¶å¯åœ¨çº¯ CPU ç¯å¢ƒä¸‹æé€Ÿæ¨ç†ï¼›æä¾›å¸¦éŸ³é¢‘ä¸é’¢ç´å·å¸˜åŠ¨ç”»çš„ React äº¤äº’ç•Œé¢ä¸ FastAPI åç«¯æ”¯æ’‘ã€‚

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

ç³»ç»Ÿåˆ†ä¸ºå››ä¸ªæ ¸å¿ƒå±‚æ¬¡ï¼š
1. **æ•°æ®å±‚**: MIDI è§£æ â†’ REMI ç¼–ç  â†’ å°è£… Dataset (`pretty_midi`, `miditok`)
2. **æ¨¡å‹å±‚**: Transformer Decoder ä»é›¶å®ç° (çº¯æ‰‹å·¥ PyTorch å®ç°ï¼Œä¸ä½¿ç”¨ `nn.Transformer`)
3. **æ¨ç†å±‚**: æ¡ä»¶é‡‡æ · â†’ MIDI ç”Ÿæˆ (æ”¯æŒ CPU æ¨ç†ï¼Œé€šè¿‡ KV Cache åŠ é€Ÿ 5~10 å€)
4. **å±•ç¤ºå±‚**: Web ç•Œé¢äº¤äº’ä¸å¯è§†åŒ–åº”ç”¨ (React, FastAPI, Vite, `html-midi-player`)

### æ¨¡å‹è¶…å‚æ•°
- **å±‚æ•° (n_layers)**: 6
- **æ³¨æ„åŠ›å¤´æ•° (n_heads)**: 8
- **éšå±‚ç»´åº¦ (d_model)**: 512
- **æœ€å¤§åºåˆ—é•¿åº¦**: 1024
- **è¯è¡¨å¤§å°**: çº¦ 400 (REMI Token + æ¡ä»¶å‰ç¼€ Token)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/yourusername/StyleMIDI.git
cd StyleMIDI

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### è¿è¡Œ Web ç•Œé¢è¿›è¡Œæ¨ç†

é¡¹ç›®å±•ç¤ºé‡‡ç”¨å‰åç«¯åˆ†ç¦»æ¶æ„ï¼ˆå‰ç«¯ React + Viteï¼Œåç«¯ FastAPIï¼‰ï¼š

**1. å¯åŠ¨åç«¯ API æœåŠ¡**
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œ
python api/server.py
```
*æœåŠ¡é»˜è®¤è¿è¡Œåœ¨ http://localhost:8000*

**2. å¯åŠ¨å‰ç«¯ UI**
```bash
cd style_midi_ui
npm install
npm run dev
```
*åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æç¤ºçš„æœ¬åœ°åœ°å€ (ä¾‹å¦‚ http://localhost:5173)ï¼Œå³å¯åœ¨é¡µé¢ä¸­é€‰æ‹©ä½œæ›²å®¶ã€è°ƒèŠ‚é€Ÿåº¦/åŠ›åº¦ç­‰è¿ç»­å€¼æ¡ä»¶ï¼Œä¸€é”®ç”Ÿæˆé£æ ¼åŒ–æ›²ç›®ï¼*

### æ¨¡å‹è®­ç»ƒ

ä½¿ç”¨ MAESTRO æ•°æ®é›†ä»å¤´å¼€å§‹è®­ç»ƒï¼š

```bash
# è‡ªåŠ¨åŒ–ç‰¹å¾æå–ä¸æ•°æ®å‡†å¤‡ (å«è‡ªåŠ¨ä¸‹è½½ã€ç‰¹å¾è®¡ç®—åŠ token ç”Ÿæˆ)
python scripts/prepare_data.py

# å¯åŠ¨æ··åˆç²¾åº¦è®­ç»ƒ (è®­ç»ƒå‚è€ƒåŠæ—¥å¿—è§ result/style_midi_train.ipynb)
python src/train.py
```
